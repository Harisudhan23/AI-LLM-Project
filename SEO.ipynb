{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    How AI is Transforming SEO\n",
      "\n",
      "Artificial Intelligence (AI) is rapidly changing the landscape of Search Engine Optimization (SEO). Here's how AI is revolutionizing SEO:\n",
      "\n",
      "AI-Powered Keyword Research: AI tools analyze vast data to identify trending keywords, helping businesses target the right keywords for better search engine rankings.\n",
      "\n",
      "Content Optimization: AI assists in content creation by suggesting improvements for readability and SEO, ensuring content is engaging and search engine-friendly.\n",
      "\n",
      "Enhanced User Experience: AI analyzes user behavior on websites to help businesses improve navigation and design, creating a seamless experience for visitors.\n",
      "\n",
      "Predictive Analytics: AI predicts SEO trends and outcomes, allowing businesses to stay ahead of competitors and adjust their strategies proactively.\n",
      "\n",
      "AI is reshaping SEO by making it smarter, faster, and more effective. Embracing these tools can give businesses a significant edge in improving their online presence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blog_text = \"\"\"\n",
    "    How AI is Transforming SEO\n",
    "\n",
    "Artificial Intelligence (AI) is rapidly changing the landscape of Search Engine Optimization (SEO). Here's how AI is revolutionizing SEO:\n",
    "\n",
    "AI-Powered Keyword Research: AI tools analyze vast data to identify trending keywords, helping businesses target the right keywords for better search engine rankings.\n",
    "\n",
    "Content Optimization: AI assists in content creation by suggesting improvements for readability and SEO, ensuring content is engaging and search engine-friendly.\n",
    "\n",
    "Enhanced User Experience: AI analyzes user behavior on websites to help businesses improve navigation and design, creating a seamless experience for visitors.\n",
    "\n",
    "Predictive Analytics: AI predicts SEO trends and outcomes, allowing businesses to stay ahead of competitors and adjust their strategies proactively.\n",
    "\n",
    "AI is reshaping SEO by making it smarter, faster, and more effective. Embracing these tools can give businesses a significant edge in improving their online presence.\n",
    "\"\"\"\n",
    "\n",
    "# Print input blog text (optional)\n",
    "print(blog_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: click in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
      "langchain-community 0.3.5 requires numpy<2.0.0,>=1.26.0; python_version >= \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
      "spacy-transformers 1.3.5 requires transformers<4.37.0,>=3.4.0, but you have transformers 4.47.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 2.6/12.8 MB 15.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.6/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 17.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 17.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harihara Sudhan\n",
      "[nltk_data]     N\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'transforming', 'seo', 'artificial', 'intelligence', 'ai', 'rapidly', 'changing', 'landscape', 'search', 'engine', 'optimization', 'seo', \"'s\", 'ai', 'revolutionizing', 'seo', 'ai', 'powered', 'keyword', 'research', 'ai', 'tools', 'analyze', 'vast', 'data', 'identify', 'trending', 'keywords', 'helping', 'businesses', 'target', 'right', 'keywords', 'better', 'search', 'engine', 'rankings', 'content', 'optimization', 'ai', 'assists', 'content', 'creation', 'suggesting', 'improvements', 'readability', 'seo', 'ensuring', 'content', 'engaging', 'search', 'engine', 'friendly', 'enhanced', 'user', 'experience', 'ai', 'analyzes', 'user', 'behavior', 'websites', 'help', 'businesses', 'improve', 'navigation', 'design', 'creating', 'seamless', 'experience', 'visitors', 'predictive', 'analytics', 'ai', 'predicts', 'seo', 'trends', 'outcomes', 'allowing', 'businesses', 'stay', 'ahead', 'competitors', 'adjust', 'strategies', 'proactively', 'ai', 'reshaping', 'seo', 'making', 'smarter', 'faster', 'effective', 'embracing', 'tools', 'give', 'businesses', 'significant', 'edge', 'improving', 'online', 'presence']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Initialize SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean and tokenize blog text\n",
    "def clean_and_tokenize(text):\n",
    "    # Remove punctuation and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.strip().lower() for token in doc if token.text.strip().lower() not in stop_words and token.text.strip() not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Clean and tokenize the blog text\n",
    "cleaned_blog_text = clean_and_tokenize(blog_text)\n",
    "print(cleaned_blog_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_keyword_density(self, text, target_keyword):\n",
    "        words = text.lower().split()\n",
    "        word_count = len(words)\n",
    "        keyword_count = words.count(target_keyword.lower())\n",
    "        keyword_density = (keyword_count / word_count) * 100 if word_count > 0 else 0\n",
    "        return keyword_density, keyword_count, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ai': 8.823529411764707, 'transforming': 0.9803921568627451, 'seo': 5.88235294117647, 'artificial': 0.9803921568627451, 'intelligence': 0.9803921568627451, 'rapidly': 0.9803921568627451, 'changing': 0.9803921568627451, 'landscape': 0.9803921568627451, 'search': 2.941176470588235, 'engine': 2.941176470588235, 'optimization': 1.9607843137254901, \"'s\": 0.9803921568627451, 'revolutionizing': 0.9803921568627451, 'powered': 0.9803921568627451, 'keyword': 0.9803921568627451, 'research': 0.9803921568627451, 'tools': 1.9607843137254901, 'analyze': 0.9803921568627451, 'vast': 0.9803921568627451, 'data': 0.9803921568627451, 'identify': 0.9803921568627451, 'trending': 0.9803921568627451, 'keywords': 1.9607843137254901, 'helping': 0.9803921568627451, 'businesses': 3.9215686274509802, 'target': 0.9803921568627451, 'right': 0.9803921568627451, 'better': 0.9803921568627451, 'rankings': 0.9803921568627451, 'content': 2.941176470588235, 'assists': 0.9803921568627451, 'creation': 0.9803921568627451, 'suggesting': 0.9803921568627451, 'improvements': 0.9803921568627451, 'readability': 0.9803921568627451, 'ensuring': 0.9803921568627451, 'engaging': 0.9803921568627451, 'friendly': 0.9803921568627451, 'enhanced': 0.9803921568627451, 'user': 1.9607843137254901, 'experience': 1.9607843137254901, 'analyzes': 0.9803921568627451, 'behavior': 0.9803921568627451, 'websites': 0.9803921568627451, 'help': 0.9803921568627451, 'improve': 0.9803921568627451, 'navigation': 0.9803921568627451, 'design': 0.9803921568627451, 'creating': 0.9803921568627451, 'seamless': 0.9803921568627451, 'visitors': 0.9803921568627451, 'predictive': 0.9803921568627451, 'analytics': 0.9803921568627451, 'predicts': 0.9803921568627451, 'trends': 0.9803921568627451, 'outcomes': 0.9803921568627451, 'allowing': 0.9803921568627451, 'stay': 0.9803921568627451, 'ahead': 0.9803921568627451, 'competitors': 0.9803921568627451, 'adjust': 0.9803921568627451, 'strategies': 0.9803921568627451, 'proactively': 0.9803921568627451, 'reshaping': 0.9803921568627451, 'making': 0.9803921568627451, 'smarter': 0.9803921568627451, 'faster': 0.9803921568627451, 'effective': 0.9803921568627451, 'embracing': 0.9803921568627451, 'give': 0.9803921568627451, 'significant': 0.9803921568627451, 'edge': 0.9803921568627451, 'improving': 0.9803921568627451, 'online': 0.9803921568627451, 'presence': 0.9803921568627451}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to calculate keyword density\n",
    "def calculate_keyword_density(tokens):\n",
    "    # Count frequency of each word\n",
    "    word_count = Counter(tokens)\n",
    "    total_words = sum(word_count.values())\n",
    "    density = {word:(count / total_words)*100 for word, count in word_count.items()}\n",
    "    return density\n",
    "\n",
    "# Calculate keyword density for the blog text\n",
    "keyword_density = calculate_keyword_density(cleaned_blog_text)\n",
    "print(keyword_density)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textstat) (75.6.0)\n",
      "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
      "Downloading pyphen-0.17.0-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 16.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.17.0 textstat-0.7.4\n"
     ]
    }
   ],
   "source": [
    "! pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harihara Sudhan N\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Harihara Sudhan N\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Harihara Sudhan N\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Cleaned_Content  Flesch_Reading_Ease  \\\n",
      "0  \\n    How AI is Transforming SEO\\n\\nArtificial...                 26.4   \n",
      "\n",
      "   Flesch_Kincaid_Grade                                       Content_Flow  \n",
      "0                  14.4  [{'label': 'POSITIVE', 'score': 0.998249948024...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Create a DataFrame with the blog text\n",
    "data = {'Cleaned_Content': [blog_text]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Assess readability\n",
    "df['Flesch_Reading_Ease'] = df['Cleaned_Content'].apply(flesch_reading_ease)\n",
    "df['Flesch_Kincaid_Grade'] = df['Cleaned_Content'].apply(flesch_kincaid_grade)\n",
    "\n",
    "# NLP flow analysis with Hugging Face\n",
    "nlp_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "df['Content_Flow'] = df['Cleaned_Content'].apply(lambda x: nlp_pipeline(x))\n",
    "\n",
    "# Display results\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Score: 26.40\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "# Calculate the readability score of the blog content\n",
    "readability_score = textstat.flesch_reading_ease(blog_text)\n",
    "print(f\"Readability Score: {readability_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H1': 1, 'H2': 1, 'H3': 1, 'H4': 0, 'H5': 0, 'H6': 0}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.22.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.151.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.25.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harihara sudhan n\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEO Recommendations:\n",
      "## SEO Improvements and Recommendations for the Blog Post:\n",
      "\n",
      "This blog post provides a good starting point but lacks depth and specific examples to truly engage readers and rank well. Here's how to improve it:\n",
      "\n",
      "**Content & Keyword Optimization:**\n",
      "\n",
      "* **Expand on each AI application:**  Instead of just stating what AI does, provide concrete examples and benefits.  For \"AI-Powered Keyword Research,\" mention specific tools (e.g., SEMrush, Ahrefs) and how their AI features help.  For \"Content Optimization,\" discuss elements like semantic SEO, content structure analysis, and examples of AI-powered writing assistants (e.g., Jasper, Copy.ai).  For \"Enhanced User Experience,\" talk about personalization, chatbot integration, and A/B testing powered by AI. For \"Predictive Analytics,\" provide examples like forecasting keyword search volume or predicting algorithm updates' impact.\n",
      "* **Target Long-Tail Keywords:**  Instead of focusing solely on broad keywords like \"AI\" and \"SEO,\" incorporate long-tail keywords that address specific user queries. Examples include \"best AI tools for keyword research,\" \"how to use AI for content optimization,\" or \"AI-driven UX improvements for e-commerce.\"\n",
      "* **Include Internal and External Links:**  Link to other relevant articles on your website (internal links) and authoritative external resources (external links). This improves site navigation, user experience, and SEO.\n",
      "* **Add Data and Statistics:**  Back up your claims with data and statistics to add credibility and demonstrate the impact of AI on SEO.  For example, \"AI-powered content optimization tools can increase organic traffic by X%.\"\n",
      "* **Use Header Tags (H2, H3, etc.):**  Structure the content with header tags to improve readability and SEO.  Each AI application could be an H2, with specific examples and benefits under H3s.\n",
      "* **Optimize Meta Description:**  Craft a compelling meta description that accurately summarizes the content and encourages clicks from search results.  Include relevant keywords.\n",
      "* **Optimize Title Tag:**  The current title is okay, but could be more specific. Consider something like \"How AI is Transforming SEO: Tools and Strategies for Success\" or \"The Future of SEO: How AI is Revolutionizing Search Engine Optimization.\"\n",
      "\n",
      "\n",
      "**Technical SEO:**\n",
      "\n",
      "* **Optimize Images:** Use descriptive alt text for images to improve accessibility and SEO.  Compress images to reduce page load time.\n",
      "* **Ensure Mobile-Friendliness:**  The blog post should be responsive and display correctly on all devices.\n",
      "* **Page Speed Optimization:**  Minimize page load time by optimizing images, code, and server response time.\n",
      "\n",
      "\n",
      "**User Experience (UX):**\n",
      "\n",
      "* **Add Visuals:** Break up the text with relevant images, videos, or infographics to enhance engagement.\n",
      "* **Improve Readability:** Use shorter paragraphs, bullet points, and clear formatting to make the content easier to digest.\n",
      "* **Add a Call to Action (CTA):** Encourage readers to take action, such as subscribing to your newsletter, downloading a resource, or contacting you for a consultation.\n",
      "\n",
      "\n",
      "**Promotion and Distribution:**\n",
      "\n",
      "* **Share on Social Media:** Promote the blog post on relevant social media platforms.\n",
      "* **Email Marketing:**  Send the blog post to your email subscribers.\n",
      "* **Outreach:**  Reach out to other bloggers and influencers in the SEO space to share your content.\n",
      "\n",
      "\n",
      "By implementing these SEO improvements, you can significantly increase the visibility, engagement, and overall effectiveness of your blog post.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "# Configure the genai API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDR0Sr1VV1TJl3AFNScIdubB7JkyUsJhSo\"\n",
    "\n",
    "# Initialize the genai model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-002\")\n",
    "\n",
    "# Define the prompt template for SEO recommendations\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"Based on the blog content, suggest SEO improvements and recommendations:\\n\\n{content}\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain with the genai model and prompt template\n",
    "seo_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Example blog content\n",
    "#blog_content = \"In this blog, we explore the top 10 tips for improving your website's SEO performance in 2024. From keyword optimization to backlink strategies, these actionable steps can help you rank higher in search engine results.\"\n",
    "\n",
    "# Generate SEO recommendations\n",
    "seo_recommendations = seo_chain.run(content=blog_text)\n",
    "\n",
    "# Output the SEO recommendations\n",
    "print(\"SEO Recommendations:\")\n",
    "print(seo_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatSession', 'GenerationConfig', 'GenerativeModel', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'annotations', 'api_key', 'caching', 'configure', 'create_tuned_model', 'delete_file', 'delete_tuned_model', 'embed_content', 'embed_content_async', 'get_base_model', 'get_file', 'get_model', 'get_operation', 'get_tuned_model', 'list_files', 'list_models', 'list_operations', 'list_tuned_models', 'protos', 'responder', 'string_utils', 'types', 'update_tuned_model', 'upload_file', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "print(dir(genai))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.generativeai' has no attribute 'generate_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the best practices for optimizing blog SEO?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'google.generativeai' has no attribute 'generate_text'"
     ]
    }
   ],
   "source": [
    "response = genai.generate_text(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    prompt=\"What are the best practices for optimizing blog SEO?\"\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
